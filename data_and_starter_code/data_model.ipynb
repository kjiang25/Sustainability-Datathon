{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# STEP 0: import libraries\n",
    "########################################\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "import sklearn.decomposition\n",
    "import sklearn.discriminant_analysis\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "import sklearn.neural_network\n",
    "import sklearn.model_selection\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.neighbors\n",
    "import sklearn.preprocessing\n",
    "import sklearn.random_projection\n",
    "import sklearn.tree\n",
    "import sklearn.svm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('./') / 'data'\n",
    "train = pd.read_csv(data_path / \"train.csv\")\n",
    "client = pd.read_csv(data_path / \"client.csv\")\n",
    "historical_weather = pd.read_csv(data_path / \"historical_weather.csv\")\n",
    "forecast_weather = pd.read_csv(data_path / \"forecast_weather.csv\")\n",
    "electricity = pd.read_csv(data_path / \"electricity_prices.csv\")\n",
    "gas = pd.read_csv(data_path / \"gas_prices.csv\")\n",
    "\n",
    "location = (pd.read_csv(data_path / \"county_lon_lats.csv\")\n",
    "            .drop(columns=[\"Unnamed: 0\"])\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureProcessorClass():\n",
    "    def __init__(self):         \n",
    "        # Columns to join on for the different datasets\n",
    "        self.weather_join = ['datetime', 'county', 'data_block_id']\n",
    "        self.gas_join = ['data_block_id']\n",
    "        self.electricity_join = ['datetime', 'data_block_id']\n",
    "        self.client_join = ['county', 'is_business', 'product_type', 'data_block_id']\n",
    "        \n",
    "        # Columns of latitude & longitude\n",
    "        self.lat_lon_columns = ['latitude', 'longitude']\n",
    "        \n",
    "        # Aggregate stats \n",
    "        self.agg_stats = ['mean'] #, 'min', 'max', 'std', 'median']\n",
    "        \n",
    "        # Categorical columns (specify for XGBoost)\n",
    "        self.category_columns = ['county', 'is_business', 'product_type', 'is_consumption', 'data_block_id']\n",
    "\n",
    "    def create_new_column_names(self, cons, suffix, columns_no_change):\n",
    "        '''Change column names by given suffix, keep columns_no_change, and return back the data'''\n",
    "        cons.columns = [col + suffix \n",
    "                      if col not in columns_no_change\n",
    "                      else col\n",
    "                      for col in cons.columns\n",
    "                      ]\n",
    "        return cons \n",
    "\n",
    "    def flatten_multi_index_columns(self, cons):\n",
    "        cons.columns = ['_'.join([col for col in multi_col if len(col)>0]) \n",
    "                      for multi_col in cons.columns]\n",
    "        return cons\n",
    "    \n",
    "    def create_data_features(self, data):\n",
    "        '''📊Create features for main data (test or train) set📊'''\n",
    "        # To datetime\n",
    "        data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "        \n",
    "        # Time period features\n",
    "        data['date'] = data['datetime'].dt.normalize()\n",
    "        data['year'] = data['datetime'].dt.year\n",
    "        data['quarter'] = data['datetime'].dt.quarter\n",
    "        data['month'] = data['datetime'].dt.month\n",
    "        data['week'] = data['datetime'].dt.isocalendar().week\n",
    "        data['hour'] = data['datetime'].dt.hour\n",
    "        \n",
    "        # Day features\n",
    "        data['day_of_year'] = data['datetime'].dt.day_of_year\n",
    "        data['day_of_month']  = data['datetime'].dt.day\n",
    "        data['day_of_week'] = data['datetime'].dt.day_of_week\n",
    "        return data\n",
    "\n",
    "    def create_client_features(self, client):\n",
    "        '''💼 Create client features 💼'''\n",
    "        # Modify column names - specify suffix\n",
    "        client = self.create_new_column_names(client, \n",
    "                                           suffix='_client',\n",
    "                                           columns_no_change = self.client_join\n",
    "                                          )       \n",
    "        return client\n",
    "    \n",
    "    def create_historical_weather_features(self, historical_weather):\n",
    "        '''⌛🌤️ Create historical weather features 🌤️⌛'''\n",
    "        \n",
    "        # To datetime\n",
    "        historical_weather['datetime'] = pd.to_datetime(historical_weather['datetime'])\n",
    "        \n",
    "        # Add county\n",
    "        historical_weather[self.lat_lon_columns] = historical_weather[self.lat_lon_columns].astype(float).round(1)\n",
    "        historical_weather = historical_weather.merge(location, how = 'left', on = self.lat_lon_columns)\n",
    "\n",
    "        # Modify column names - specify suffix\n",
    "        historical_weather = self.create_new_column_names(historical_weather,\n",
    "                                                          suffix='_h',\n",
    "                                                          columns_no_change = self.lat_lon_columns + self.weather_join\n",
    "                                                          ) \n",
    "        \n",
    "        # Group by & calculate aggregate stats \n",
    "        agg_columns = [col for col in historical_weather.columns if col not in self.lat_lon_columns + self.weather_join]\n",
    "        agg_dict = {agg_col: self.agg_stats for agg_col in agg_columns}\n",
    "        historical_weather = historical_weather.groupby(self.weather_join).agg(agg_dict).reset_index() \n",
    "        \n",
    "        # Flatten the multi column aggregates\n",
    "        historical_weather = self.flatten_multi_index_columns(historical_weather) \n",
    "        \n",
    "        # Test set has 1 day offset for hour<11 and 2 day offset for hour>11\n",
    "        historical_weather['hour_h'] = historical_weather['datetime'].dt.hour\n",
    "        historical_weather['datetime'] = (historical_weather\n",
    "                                               .apply(lambda x: \n",
    "                                                      x['datetime'] + pd.DateOffset(1) \n",
    "                                                      if x['hour_h']< 11 \n",
    "                                                      else x['datetime'] + pd.DateOffset(2),\n",
    "                                                      axis=1)\n",
    "                                              )\n",
    "        \n",
    "        return historical_weather\n",
    "    \n",
    "    def create_forecast_weather_features(self, forecast_weather):\n",
    "        '''🔮🌤️ Create forecast weather features 🌤️🔮'''\n",
    "        \n",
    "        # Rename column and drop\n",
    "        forecast_weather = (forecast_weather\n",
    "                            .rename(columns = {'forecast_datetime': 'datetime'})\n",
    "                            .drop(columns = 'origin_datetime') # not needed\n",
    "                           )\n",
    "        \n",
    "        # To datetime\n",
    "        forecast_weather['datetime'] = (pd.to_datetime(forecast_weather['datetime'])\n",
    "                                        .dt\n",
    "                                        .tz_localize(None)\n",
    "                                       )\n",
    "\n",
    "        # Add county\n",
    "        forecast_weather[self.lat_lon_columns] = forecast_weather[self.lat_lon_columns].astype(float).round(1)\n",
    "        forecast_weather = forecast_weather.merge(location, how = 'left', on = self.lat_lon_columns)\n",
    "        \n",
    "        # Modify column names - specify suffix\n",
    "        forecast_weather = self.create_new_column_names(forecast_weather,\n",
    "                                                        suffix='_f',\n",
    "                                                        columns_no_change = self.lat_lon_columns + self.weather_join\n",
    "                                                        ) \n",
    "        \n",
    "        # Group by & calculate aggregate stats \n",
    "        agg_columns = [col for col in forecast_weather.columns if col not in self.lat_lon_columns + self.weather_join]\n",
    "        agg_dict = {agg_col: self.agg_stats for agg_col in agg_columns}\n",
    "        forecast_weather = forecast_weather.groupby(self.weather_join).agg(agg_dict).reset_index() \n",
    "        \n",
    "        # Flatten the multi column aggregates\n",
    "        forecast_weather = self.flatten_multi_index_columns(forecast_weather)     \n",
    "        return forecast_weather\n",
    "\n",
    "    def create_electricity_features(self, electricity):\n",
    "        '''⚡ Create electricity prices features ⚡'''\n",
    "        # To datetime\n",
    "        electricity['forecast_date'] = pd.to_datetime(electricity['forecast_date'])\n",
    "        \n",
    "        # Test set has 1 day offset\n",
    "        electricity['datetime'] = electricity['forecast_date'] + pd.DateOffset(1)\n",
    "        \n",
    "        # Modify column names - specify suffix\n",
    "        electricity = self.create_new_column_names(electricity, \n",
    "                                                   suffix='_electricity',\n",
    "                                                   columns_no_change = self.electricity_join\n",
    "                                                  )             \n",
    "        return electricity\n",
    "\n",
    "    def create_gas_features(self, gas):\n",
    "        '''⛽ Create gas prices features ⛽'''\n",
    "        # Mean gas price\n",
    "        gas['mean_price_per_mwh'] = (gas['lowest_price_per_mwh'] + gas['highest_price_per_mwh'])/2\n",
    "        \n",
    "        # Modify column names - specify suffix\n",
    "        gas = self.create_new_column_names(gas, \n",
    "                                           suffix='_gas',\n",
    "                                           columns_no_change = self.gas_join\n",
    "                                          )       \n",
    "        return gas\n",
    "    \n",
    "    def __call__(self, data, client, historical_weather, forecast_weather, electricity, gas):\n",
    "        '''Processing of features from all datasets, merge together and return features for dataframe cons '''\n",
    "        # Create features for relevant dataset\n",
    "        data = self.create_data_features(data)\n",
    "        client = self.create_client_features(client)\n",
    "        historical_weather = self.create_historical_weather_features(historical_weather)\n",
    "        forecast_weather = self.create_forecast_weather_features(forecast_weather)\n",
    "        electricity = self.create_electricity_features(electricity)\n",
    "        gas = self.create_gas_features(gas)\n",
    "        \n",
    "        # 🔗 Merge all datasets into one cons 🔗\n",
    "        cons = data.merge(client, how='left', on = self.client_join)\n",
    "        cons = cons.merge(historical_weather, how='left', on = self.weather_join)\n",
    "        cons = cons.merge(forecast_weather, how='left', on = self.weather_join)\n",
    "        cons = cons.merge(electricity, how='left', on = self.electricity_join)\n",
    "        cons = cons.merge(gas, how='left', on = self.gas_join)\n",
    "        \n",
    "        # Change columns to categorical for XGBoost\n",
    "        cons[self.category_columns] = cons[self.category_columns].astype('category')\n",
    "        return cons\n",
    "    \n",
    "def create_revealed_targets_train(data, N_day_lags):\n",
    "    '''🎯 Create past revealed_targets for train set based on number of day lags N_day_lags 🎯 '''    \n",
    "    original_datetime = data['datetime']\n",
    "    revealed_targets = data[['datetime', 'prediction_unit_id', 'is_consumption', 'target']].copy()\n",
    "    \n",
    "    # Create revealed targets for all day lags\n",
    "    for day_lag in range(2, N_day_lags+1):\n",
    "        revealed_targets['datetime'] = original_datetime + pd.DateOffset(day_lag)\n",
    "        data = data.merge(revealed_targets, \n",
    "                          how='left', \n",
    "                          on = ['datetime', 'prediction_unit_id', 'is_consumption'],\n",
    "                          suffixes = ('', f'_{day_lag}_days_ago')\n",
    "                         )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features for training data\n",
    "# NOTE: this can take a little bit to run\n",
    "FeatureProcessor = FeatureProcessorClass()\n",
    "N_day_lags = 15  # incorporate data from historical 15 days\n",
    "\n",
    "\n",
    "data = FeatureProcessor(data = train.copy(),\n",
    "                      client = client.copy(),\n",
    "                      historical_weather = historical_weather.copy(),\n",
    "                      forecast_weather = forecast_weather.copy(),\n",
    "                      electricity = electricity.copy(),\n",
    "                      gas = gas.copy(),\n",
    "                     )\n",
    "\n",
    "cons = create_revealed_targets_train(data.copy(), \n",
    "                                  N_day_lags = N_day_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features to CSV\n",
    "cons.to_csv('train_features.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2018352, 71)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['county', 'is_business', 'product_type', 'target', 'is_consumption',\n",
       "       'datetime', 'data_block_id', 'row_id', 'prediction_unit_id', 'date',\n",
       "       'year', 'quarter', 'month', 'week', 'hour', 'day_of_year',\n",
       "       'day_of_month', 'day_of_week', 'eic_count_client',\n",
       "       'installed_capacity_client', 'date_client', 'temperature_h_mean',\n",
       "       'dewpoint_h_mean', 'rain_h_mean', 'snowfall_h_mean',\n",
       "       'surface_pressure_h_mean', 'cloudcover_total_h_mean',\n",
       "       'cloudcover_low_h_mean', 'cloudcover_mid_h_mean',\n",
       "       'cloudcover_high_h_mean', 'windspeed_10m_h_mean',\n",
       "       'winddirection_10m_h_mean', 'shortwave_radiation_h_mean',\n",
       "       'direct_solar_radiation_h_mean', 'diffuse_radiation_h_mean', 'hour_h',\n",
       "       'hours_ahead_f_mean', 'temperature_f_mean', 'dewpoint_f_mean',\n",
       "       'cloudcover_high_f_mean', 'cloudcover_low_f_mean',\n",
       "       'cloudcover_mid_f_mean', 'cloudcover_total_f_mean',\n",
       "       '10_metre_u_wind_component_f_mean', '10_metre_v_wind_component_f_mean',\n",
       "       'direct_solar_radiation_f_mean',\n",
       "       'surface_solar_radiation_downwards_f_mean', 'snowfall_f_mean',\n",
       "       'total_precipitation_f_mean', 'forecast_date_electricity',\n",
       "       'euros_per_mwh_electricity', 'origin_date_electricity',\n",
       "       'forecast_date_gas', 'lowest_price_per_mwh_gas',\n",
       "       'highest_price_per_mwh_gas', 'origin_date_gas',\n",
       "       'mean_price_per_mwh_gas', 'target_2_days_ago', 'target_3_days_ago',\n",
       "       'target_4_days_ago', 'target_5_days_ago', 'target_6_days_ago',\n",
       "       'target_7_days_ago', 'target_8_days_ago', 'target_9_days_ago',\n",
       "       'target_10_days_ago', 'target_11_days_ago', 'target_12_days_ago',\n",
       "       'target_13_days_ago', 'target_14_days_ago', 'target_15_days_ago'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1009176, 71), (1009176, 71))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons = cons[cons['is_consumption'] == 1].copy()\n",
    "prod = cons[cons['is_consumption'] == 0].copy()\n",
    "cons.shape, prod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "county: 16 unique values\n",
      "is_business: 2 unique values\n",
      "product_type: 4 unique values\n",
      "is_consumption: 1 unique values\n",
      "data_block_id: 638 unique values\n",
      "date_client: 636 unique values\n",
      "origin_date_electricity: 15286 unique values\n",
      "forecast_date_gas: 637 unique values\n",
      "origin_date_gas: 637 unique values\n"
     ]
    }
   ],
   "source": [
    "for col in cons.select_dtypes(include=['object', 'category']).columns:\n",
    "    print(f\"{col}: {cons[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['data_block_id', 'date_client', 'origin_date_electricity', 'forecast_date_gas', 'origin_date_gas']  # Replace with actual column names\n",
    "cons = cons.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cons.shape=(1009176, 84)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########################################\n",
    "# STEP 2: Apply \"non-learned\" data transformations\n",
    "########################################\n",
    "\n",
    "# Limited to these 4 feature transformations\n",
    "\n",
    "# one hot encode categorical columns (increases vc dim)\n",
    "cons = pd.get_dummies(cons)\n",
    "print(f\"cons.shape={cons.shape}\")\n",
    "\n",
    "# convert non-numeric columns to numeric (no effect on vd dim)\n",
    "# le = sklearn.preprocessing.LabelEncoder()\n",
    "# cons = cons[cons.columns[:]].apply(le.fit_transform)\n",
    "# print(f\"cons.shape={cons.shape}\")\n",
    "\n",
    "# apply the polynomial feature map (increases vc dim)\n",
    "# poly = sklearn.preprocessing.PolynomialFeatures(3)\n",
    "# cons = poly.fit_transform(cons)\n",
    "# print(f\"cons.shape={cons.shape}\")\n",
    "\n",
    "# apply a random projection (decreases vc dim)\n",
    "# proj = sklearn.random_projection.GaussianRandomProjection(\n",
    "#     n_components=120, # output dimension\n",
    "#     random_state=42,\n",
    "#     )\n",
    "# cons = proj.fit_transform(cons)\n",
    "# print(f\"cons.shape={cons.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'target'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [94]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m########################################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# STEP 3: Create train/test sets\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m########################################\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[43mcons\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m/\u001b[39mcons[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstalled_capacity_client\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m cons \u001b[38;5;241m=\u001b[39m cons\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstalled_capacity_client\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m train_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.75\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'target'"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# STEP 3: Create train/test sets\n",
    "########################################\n",
    "\n",
    "target = cons['target']/cons['installed_capacity_client']\n",
    "cons = cons.drop(columns=['target', 'installed_capacity_client'])\n",
    "\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# ensure that the ratios sum to 1.0\n",
    "epsilon = 1e-10\n",
    "assert(1 - epsilon <= train_ratio + validation_ratio + test_ratio <= 1 + epsilon)\n",
    "\n",
    "# create train0/test set\n",
    "x_train0, x_test, y_train0, y_test = sklearn.model_selection.train_test_split(\n",
    "    cons,\n",
    "    target,\n",
    "    test_size=test_ratio,\n",
    "    random_state=0,\n",
    "    )\n",
    "print(f\"len(x_train0)={len(x_train0)}\")\n",
    "print(f\"len(x_test)={len(x_test)}\")\n",
    "\n",
    "# create train/validation set\n",
    "x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(\n",
    "    x_train0,\n",
    "    y_train0,\n",
    "    test_size=validation_ratio/(train_ratio + validation_ratio),\n",
    "    random_state=0,\n",
    "    )\n",
    "print(f\"len(x_train)={len(x_train)}\")\n",
    "print(f\"len(x_val)={len(x_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess datetime columns\n",
    "datetime_columns = ['datetime', 'date', 'forecast_date_electricity']  # Replace with actual datetime column names\n",
    "\n",
    "# Convert datetime columns to numeric features (e.g., year, month, day)\n",
    "for col in datetime_columns:\n",
    "    if col in x_train.columns:\n",
    "        x_train[col + '_year'] = x_train[col].dt.year\n",
    "        x_train[col + '_month'] = x_train[col].dt.month\n",
    "        x_train[col + '_day'] = x_train[col].dt.day\n",
    "        x_train = x_train.drop(columns=[col])  # Drop the original datetime column\n",
    "\n",
    "        x_val[col + '_year'] = x_val[col].dt.year\n",
    "        x_val[col + '_month'] = x_val[col].dt.month\n",
    "        x_val[col + '_day'] = x_val[col].dt.day\n",
    "        x_val = x_val.drop(columns=[col])\n",
    "\n",
    "        x_test[col + '_year'] = x_test[col].dt.year\n",
    "        x_test[col + '_month'] = x_test[col].dt.month\n",
    "        x_test[col + '_day'] = x_test[col].dt.day\n",
    "        x_test = x_test.drop(columns=[col])\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "x_train = x_train.apply(pd.to_numeric, errors='coerce')\n",
    "x_val = x_val.apply(pd.to_numeric, errors='coerce')\n",
    "x_test = x_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices with NaN in y_train, y_val, y_test\n",
    "nan_indices_train = y_train[y_train.isna()].index\n",
    "nan_indices_val = y_val[y_val.isna()].index\n",
    "nan_indices_test = y_test[y_test.isna()].index\n",
    "\n",
    "# Drop the corresponding rows from features and target\n",
    "x_train = x_train.drop(index=nan_indices_train)\n",
    "y_train = y_train.drop(index=nan_indices_train)\n",
    "\n",
    "x_val = x_val.drop(index=nan_indices_val)\n",
    "y_val = y_val.drop(index=nan_indices_val)\n",
    "\n",
    "x_test = x_test.drop(index=nan_indices_test)\n",
    "y_test = y_test.drop(index=nan_indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "# Fill NaN with mean/median/mode for each column\n",
    "for col in x_train.columns:\n",
    "    # For numerical columns, use mean or median\n",
    "    if x_train[col].dtype in ['int64', 'float64']:\n",
    "        # Use median for robustness against outliers\n",
    "        median_value = x_train[col].median()\n",
    "        x_train[col] = x_train[col].fillna(median_value)\n",
    "        x_val[col] = x_val[col].fillna(median_value)\n",
    "        x_test[col] = x_test[col].fillna(median_value)\n",
    "    else:\n",
    "        # For categorical columns, use mode (most frequent value)\n",
    "        mode_value = x_train[col].mode()[0]\n",
    "        x_train[col] = x_train[col].fillna(mode_value)\n",
    "        x_val[col] = x_val[col].fillna(mode_value)\n",
    "        x_test[col] = x_test[col].fillna(mode_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_accuracy=0.9579\n",
      "train_accuracy=0.9734\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# STEP 5: Train a model\n",
    "########################################\n",
    "\n",
    "# NOTE:\n",
    "# the models below are listed in the order we covered them in class;\n",
    "# the parameters are listed in the order of the documentation;\n",
    "# you are responsible for understanding how all specified parameters impact the runtime and/or statistical errors\n",
    "\n",
    "# Most of our discussions in class is about \"error\"\n",
    "# accuracy = 1 - error\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    booster='gbtree',\n",
    "    objective='reg:squarederror',  # Objective for regression\n",
    "    max_depth=9,\n",
    "    min_child_weight=1,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8,\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=1,\n",
    "    reg_alpha=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_val_pred = model.predict(x_val)\n",
    "\n",
    "# Calculate R² scores\n",
    "validation_r2 = model.score(x_val, y_val)\n",
    "train_r2 = model.score(x_train, y_train)\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "validation_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "validation_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "\n",
    "# Print all metrics\n",
    "print(f\"Training metrics:\")\n",
    "print(f\"  R² Score: {train_r2:.4f}\")\n",
    "print(f\"  MAE: {train_mae:.4f}\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(\"\\nValidation metrics:\")\n",
    "print(f\"  R² Score: {validation_r2:.4f}\")\n",
    "print(f\"  MAE: {validation_mae:.4f}\")\n",
    "print(f\"  RMSE: {validation_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target mean: 0.0625, std: 0.1372\n",
      "MAE: 0.0165\n",
      "MAE as % of mean: 26.36%\n"
     ]
    }
   ],
   "source": [
    "target_mean = y_train.mean()\n",
    "target_std = y_train.std()\n",
    "print(f\"Target mean: {target_mean:.4f}, std: {target_std:.4f}\")\n",
    "print(f\"MAE: {validation_mae:.4f}\")\n",
    "print(f\"MAE as % of mean: {(validation_mae/abs(target_mean))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "county: 16 unique values\n",
      "is_business: 2 unique values\n",
      "product_type: 4 unique values\n",
      "is_consumption: 1 unique values\n",
      "data_block_id: 638 unique values\n",
      "date_client: 636 unique values\n",
      "origin_date_electricity: 15286 unique values\n",
      "forecast_date_gas: 637 unique values\n",
      "origin_date_gas: 637 unique values\n"
     ]
    }
   ],
   "source": [
    "for col in prod.select_dtypes(include=['object', 'category']).columns:\n",
    "    print(f\"{col}: {prod[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['data_block_id', 'date_client', 'origin_date_electricity', 'forecast_date_gas', 'origin_date_gas']  # Replace with actual column names\n",
    "prod = prod.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cons.shape=(1009176, 86)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########################################\n",
    "# STEP 2: Apply \"non-learned\" data transformations\n",
    "########################################\n",
    "\n",
    "# Limited to these 4 feature transformations\n",
    "\n",
    "# one hot encode categorical columns (increases vc dim)\n",
    "prod = pd.get_dummies(prod)\n",
    "print(f\"cons.shape={prod.shape}\")\n",
    "\n",
    "# convert non-numeric columns to numeric (no effect on vd dim)\n",
    "# le = sklearn.preprocessing.LabelEncoder()\n",
    "# cons = cons[cons.columns[:]].apply(le.fit_transform)\n",
    "# print(f\"cons.shape={cons.shape}\")\n",
    "\n",
    "# apply the polynomial feature map (increases vc dim)\n",
    "# poly = sklearn.preprocessing.PolynomialFeatures(3)\n",
    "# cons = poly.fit_transform(cons)\n",
    "# print(f\"cons.shape={cons.shape}\")\n",
    "\n",
    "# apply a random projection (decreases vc dim)\n",
    "# proj = sklearn.random_projection.GaussianRandomProjection(\n",
    "#     n_components=120, # output dimension\n",
    "#     random_state=42,\n",
    "#     )\n",
    "# cons = proj.fit_transform(cons)\n",
    "# print(f\"cons.shape={cons.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(x_train0)=908258\n",
      "len(x_test)=100918\n",
      "len(x_train)=756881\n",
      "len(x_val)=151377\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# STEP 3: Create train/test sets\n",
    "########################################\n",
    "\n",
    "target = prod['target']/prod['installed_capacity_client']\n",
    "prod = prod.drop(columns=['target', 'installed_capacity_client'])\n",
    "\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# ensure that the ratios sum to 1.0\n",
    "epsilon = 1e-10\n",
    "assert(1 - epsilon <= train_ratio + validation_ratio + test_ratio <= 1 + epsilon)\n",
    "\n",
    "# create train0/test set\n",
    "x_train0, x_test, y_train0, y_test = sklearn.model_selection.train_test_split(\n",
    "    prod,\n",
    "    target,\n",
    "    test_size=test_ratio,\n",
    "    random_state=0,\n",
    "    )\n",
    "print(f\"len(x_train0)={len(x_train0)}\")\n",
    "print(f\"len(x_test)={len(x_test)}\")\n",
    "\n",
    "# create train/validation set\n",
    "x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(\n",
    "    x_train0,\n",
    "    y_train0,\n",
    "    test_size=validation_ratio/(train_ratio + validation_ratio),\n",
    "    random_state=0,\n",
    "    )\n",
    "print(f\"len(x_train)={len(x_train)}\")\n",
    "print(f\"len(x_val)={len(x_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess datetime columns\n",
    "datetime_columns = ['datetime', 'date', 'forecast_date_electricity']  # Replace with actual datetime column names\n",
    "\n",
    "# Convert datetime columns to numeric features (e.g., year, month, day)\n",
    "for col in datetime_columns:\n",
    "    if col in x_train.columns:\n",
    "        x_train[col + '_year'] = x_train[col].dt.year\n",
    "        x_train[col + '_month'] = x_train[col].dt.month\n",
    "        x_train[col + '_day'] = x_train[col].dt.day\n",
    "        x_train = x_train.drop(columns=[col])  # Drop the original datetime column\n",
    "\n",
    "        x_val[col + '_year'] = x_val[col].dt.year\n",
    "        x_val[col + '_month'] = x_val[col].dt.month\n",
    "        x_val[col + '_day'] = x_val[col].dt.day\n",
    "        x_val = x_val.drop(columns=[col])\n",
    "\n",
    "        x_test[col + '_year'] = x_test[col].dt.year\n",
    "        x_test[col + '_month'] = x_test[col].dt.month\n",
    "        x_test[col + '_day'] = x_test[col].dt.day\n",
    "        x_test = x_test.drop(columns=[col])\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "x_train = x_train.apply(pd.to_numeric, errors='coerce')\n",
    "x_val = x_val.apply(pd.to_numeric, errors='coerce')\n",
    "x_test = x_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices with NaN in y_train, y_val, y_test\n",
    "nan_indices_train = y_train[y_train.isna()].index\n",
    "nan_indices_val = y_val[y_val.isna()].index\n",
    "nan_indices_test = y_test[y_test.isna()].index\n",
    "\n",
    "# Drop the corresponding rows from features and target\n",
    "x_train = x_train.drop(index=nan_indices_train)\n",
    "y_train = y_train.drop(index=nan_indices_train)\n",
    "\n",
    "x_val = x_val.drop(index=nan_indices_val)\n",
    "y_val = y_val.drop(index=nan_indices_val)\n",
    "\n",
    "x_test = x_test.drop(index=nan_indices_test)\n",
    "y_test = y_test.drop(index=nan_indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN with mean/median/mode for each column\n",
    "for col in x_train.columns:\n",
    "    # For numerical columns, use mean or median\n",
    "    if x_train[col].dtype in ['int64', 'float64']:\n",
    "        # Use median for robustness against outliers\n",
    "        median_value = x_train[col].median()\n",
    "        x_train[col] = x_train[col].fillna(median_value)\n",
    "        x_val[col] = x_val[col].fillna(median_value)\n",
    "        x_test[col] = x_test[col].fillna(median_value)\n",
    "    else:\n",
    "        # For categorical columns, use mode (most frequent value)\n",
    "        mode_value = x_train[col].mode()[0]\n",
    "        x_train[col] = x_train[col].fillna(mode_value)\n",
    "        x_val[col] = x_val[col].fillna(mode_value)\n",
    "        x_test[col] = x_test[col].fillna(mode_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrics:\n",
      "  R² Score: 0.9267\n",
      "  MAE: 0.0157\n",
      "  RMSE: 0.0372\n",
      "\n",
      "Validation metrics:\n",
      "  R² Score: 0.9109\n",
      "  MAE: 0.0165\n",
      "  RMSE: 0.0410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scikit_learn-1.5.1-py3.10-macosx-10.9-universal2.egg/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scikit_learn-1.5.1-py3.10-macosx-10.9-universal2.egg/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# STEP 5: Train a model\n",
    "########################################\n",
    "\n",
    "# NOTE:\n",
    "# the models below are listed in the order we covered them in class;\n",
    "# the parameters are listed in the order of the documentation;\n",
    "# you are responsible for understanding how all specified parameters impact the runtime and/or statistical errors\n",
    "\n",
    "# Most of our discussions in class is about \"error\"\n",
    "# accuracy = 1 - error\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    booster='gbtree',\n",
    "    objective='reg:squarederror',  # Objective for regression\n",
    "    max_depth=9,\n",
    "    min_child_weight=1,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8,\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.01,\n",
    "    reg_lambda=1,\n",
    "    reg_alpha=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(x_train)\n",
    "y_val_pred = model.predict(x_val)\n",
    "\n",
    "# Calculate R² scores\n",
    "validation_r2 = model.score(x_val, y_val)\n",
    "train_r2 = model.score(x_train, y_train)\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "validation_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "validation_rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "\n",
    "# Print all metrics\n",
    "print(f\"Training metrics:\")\n",
    "print(f\"  R² Score: {train_r2:.4f}\")\n",
    "print(f\"  MAE: {train_mae:.4f}\")\n",
    "print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "print(\"\\nValidation metrics:\")\n",
    "print(f\"  R² Score: {validation_r2:.4f}\")\n",
    "print(f\"  MAE: {validation_mae:.4f}\")\n",
    "print(f\"  RMSE: {validation_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target mean: 0.0625, std: 0.1372\n",
      "MAE: 0.0165\n",
      "MAE as % of mean: 26.36%\n"
     ]
    }
   ],
   "source": [
    "target_mean = y_train.mean()\n",
    "target_std = y_train.std()\n",
    "print(f\"Target mean: {target_mean:.4f}, std: {target_std:.4f}\")\n",
    "print(f\"MAE: {validation_mae:.4f}\")\n",
    "print(f\"MAE as % of mean: {(validation_mae/abs(target_mean))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# STEP 6: Evaluate on test set (Won't be run on test)\n",
    "########################################\n",
    "\n",
    "# WARNING:\n",
    "# this code should be run only once;\n",
    "# after the hyperparameters have been decided based on the validation performance,\n",
    "# then the False can be changed to True to run this code\n",
    "if False:\n",
    "    model.fit(x_train0, y_train0)\n",
    "    test_accuracy = model.score(x_test, y_test)\n",
    "    print(f\"test_accuracy={test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
